{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial - Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Spark Context required for any PySpark program.  Most programs will store this in a variable named `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkTutorial\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SparkSQL\n",
    "\n",
    "The following code creates a DataFrame by reading a parquet file and using SparkSQL directly from Python.\n",
    "\n",
    "> Note that this method might seem familiar to SQL Developers, however it does not have access to all of the functionality available through the PySpark interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "| policy|   make|      status|\n",
      "+-------+-------+------------+\n",
      "|CAR0001| TOYOTA|New Business|\n",
      "|CAR0001| TOYOTA|     Renewal|\n",
      "|CAR0001| TOYOTA|     Renewal|\n",
      "|CAR0002| SUBARU|New Business|\n",
      "|CAR0003|   FORD|New Business|\n",
      "|CAR0003|   FORD|     Renewal|\n",
      "|CAR0003|   FORD|     Renewal|\n",
      "|CAR0004|  MAZDA|New Business|\n",
      "|CAR0004|  MAZDA|New Business|\n",
      "|CAR0005| HOLDEN|New Business|\n",
      "|CAR0006| SUZUKI|New Business|\n",
      "|CAR0007|    BMW|New Business|\n",
      "|CAR0008|   AUDI|New Business|\n",
      "|CAR0009|  TESLA|New Business|\n",
      "|CAR0009|  TESLA|     Renewal|\n",
      "|CAR0010|HYUNDAI|New Business|\n",
      "+-------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"SELECT policy\n",
    "                       , make\n",
    "                       , CASE\n",
    "                           WHEN inception_date=start_date THEN 'New Business'\n",
    "                           ELSE 'Renewal'\n",
    "                         END AS status\n",
    "                    FROM parquet.`./data/policy.parquet`\n",
    "                      ORDER BY policy, start_date\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to save resources if you `stop()` the Spark session when you are finished.  Note that by doing this you will be unable to re-run any of the code above without first re-creating the `spark` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
