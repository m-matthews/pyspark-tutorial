{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Tutorial - Explain Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of a DataFrame involves a sequence of instructions, which can include reading data and applying transformations such as calculated columns, joins and summarisation.\n",
    "\n",
    "When an action is performed on the DataFrame, Spark analyses the code from each component and using its optimiser determines a Plan to execute the code.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "| Step | Item                   | Description |\n",
    "| ---: | :--------------------- | :---------- |\n",
    "|    1 | Parsed Logical Plan    | Initial Plan from Parsing the DataFrame code. |\n",
    "|    2 | Analyzed Logical Plan  | Analysed Plan (similar to the Parsed Logical Plan). |\n",
    "|    3 | Optimized Logical Plan | Plan created after the optimiser has processed the previous plans.  This has potentially reduced the number of instructions in the Plan significantly. |\n",
    "|    4 | Physical Plan          | The final Physical Plan that will be executed. |\n",
    "\n",
    "The following examples will show the use of the DataFrame `explain()` method to analyse the plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Sample Data\n",
    "The following code creates a DataFrame for both the policies and claims by reading CSV files and applying the data type transformations covered in previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Fixing date column 'inception_date'.\n",
      "NOTE: Fixing date column 'start_date'.\n",
      "NOTE: Fixing date column 'end_date'.\n",
      "NOTE: Fixing date column 'incident_date'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkTutorial\").getOrCreate()\n",
    "\n",
    "policyDF = spark.read.option(\"header\", True).csv(\"./data/policy.csv\") \\\n",
    "                .withColumn(\"sum_insured\", F.col(\"sum_insured\").cast(IntegerType())) \\\n",
    "                .withColumn(\"vehicle_age\", F.col(\"vehicle_age\").cast(IntegerType())) \\\n",
    "                .withColumn(\"premium\", F.col(\"premium\").cast(IntegerType()))\n",
    "claimsDF = spark.read.option(\"header\", True).csv(\"./data/claims.csv\") \\\n",
    "                .withColumn(\"cost\", F.col(\"cost\").cast(IntegerType()))\n",
    "\n",
    "def fix_dates(df):\n",
    "    \"\"\"Find all columns named *_date and convert from string to Spark Date type.\"\"\"\n",
    "    for column in df.columns:\n",
    "        if column.endswith(\"_date\") and dict(df.dtypes)[column] == 'string':\n",
    "            print(\"NOTE: Fixing date column '{}'.\".format(column))\n",
    "            df = df.withColumn(column, F.to_date(df[column], \"yyyyMMdd\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "policyDF = fix_dates(policyDF)\n",
    "claimsDF = fix_dates(claimsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the Spark Plan\n",
    "There are a number of options available to explain the different Plans used with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Physical Plan for a Spark DataFrame can be shown with the `explain()` method.  This shows the steps that will be performed when an action is taken on the given DataFrame.  The default output can be hard to review, however following steps will improve on this initial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, cast(sum_insured#19 as int) AS sum_insured#32, cast(cast(unix_timestamp(inception_date#20, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#85, cast(cast(unix_timestamp(start_date#21, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#94, cast(cast(unix_timestamp(end_date#22, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#103, cast(premium#23 as int) AS premium#50]\n",
      "+- FileScan csv [policy#16,make#17,vehicle_age#18,sum_insured#19,inception_date#20,start_date#21,end_date#22,premium#23] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/michael/pyspark-tutorial/data/policy.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<policy:string,make:string,vehicle_age:string,sum_insured:string,inception_date:string,star...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the option `mode=\"formatted\"` improves the readability with a summary before the details.  Note that this is displayed as a hierarchy with the first item (Project) being the last thing executed, and the last item (Scan) being the first (ie: the 'Project' is dependant on the 'Scan')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (2)\n",
      "+- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [8]: [policy#16, make#17, vehicle_age#18, sum_insured#19, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/michael/pyspark-tutorial/data/policy.csv]\n",
      "ReadSchema: struct<policy:string,make:string,vehicle_age:string,sum_insured:string,inception_date:string,start_date:string,end_date:string,premium:string>\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [8]: [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, cast(sum_insured#19 as int) AS sum_insured#32, cast(cast(unix_timestamp(inception_date#20, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#85, cast(cast(unix_timestamp(start_date#21, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#94, cast(cast(unix_timestamp(end_date#22, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#103, cast(premium#23 as int) AS premium#50]\n",
      "Input [8]: [policy#16, make#17, vehicle_age#18, sum_insured#19, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyDF.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of all Plans can be produced using the option `extended=True`.  In this case you can see that in the original **Parsed Logical Plan** the casting for the various columns to Integer and Date types have happened each in a separate Project layer.  The **Optimized Logical Plan** has managed to correctly reduce this to a single Project layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#85, start_date#94, to_date(end_date#22, Some(yyyyMMdd)) AS end_date#103, premium#50]\n",
      "+- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#85, to_date(start_date#21, Some(yyyyMMdd)) AS start_date#94, end_date#22, premium#50]\n",
      "   +- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, to_date(inception_date#20, Some(yyyyMMdd)) AS inception_date#85, start_date#21, end_date#22, premium#50]\n",
      "      +- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#20, start_date#21, end_date#22, cast(premium#23 as int) AS premium#50]\n",
      "         +- Project [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, sum_insured#32, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "            +- Project [policy#16, make#17, vehicle_age#18, cast(sum_insured#19 as int) AS sum_insured#32, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "               +- Relation[policy#16,make#17,vehicle_age#18,sum_insured#19,inception_date#20,start_date#21,end_date#22,premium#23] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "policy: string, make: string, vehicle_age: int, sum_insured: int, inception_date: date, start_date: date, end_date: date, premium: int\n",
      "Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#85, start_date#94, to_date(end_date#22, Some(yyyyMMdd)) AS end_date#103, premium#50]\n",
      "+- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#85, to_date(start_date#21, Some(yyyyMMdd)) AS start_date#94, end_date#22, premium#50]\n",
      "   +- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, to_date(inception_date#20, Some(yyyyMMdd)) AS inception_date#85, start_date#21, end_date#22, premium#50]\n",
      "      +- Project [policy#16, make#17, vehicle_age#41, sum_insured#32, inception_date#20, start_date#21, end_date#22, cast(premium#23 as int) AS premium#50]\n",
      "         +- Project [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, sum_insured#32, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "            +- Project [policy#16, make#17, vehicle_age#18, cast(sum_insured#19 as int) AS sum_insured#32, inception_date#20, start_date#21, end_date#22, premium#23]\n",
      "               +- Relation[policy#16,make#17,vehicle_age#18,sum_insured#19,inception_date#20,start_date#21,end_date#22,premium#23] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, cast(sum_insured#19 as int) AS sum_insured#32, cast(cast(unix_timestamp(inception_date#20, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#85, cast(cast(unix_timestamp(start_date#21, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#94, cast(cast(unix_timestamp(end_date#22, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#103, cast(premium#23 as int) AS premium#50]\n",
      "+- Relation[policy#16,make#17,vehicle_age#18,sum_insured#19,inception_date#20,start_date#21,end_date#22,premium#23] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [policy#16, make#17, cast(vehicle_age#18 as int) AS vehicle_age#41, cast(sum_insured#19 as int) AS sum_insured#32, cast(cast(unix_timestamp(inception_date#20, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#85, cast(cast(unix_timestamp(start_date#21, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#94, cast(cast(unix_timestamp(end_date#22, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#103, cast(premium#23 as int) AS premium#50]\n",
      "+- FileScan csv [policy#16,make#17,vehicle_age#18,sum_insured#19,inception_date#20,start_date#21,end_date#22,premium#23] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/michael/pyspark-tutorial/data/policy.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<policy:string,make:string,vehicle_age:string,sum_insured:string,inception_date:string,star...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policyDF.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Code\n",
    "The following example shows the transformation implemented as a single `select()` rather than using the `withColumn()` method.  This improves the original **Parsed Logical Plan** however note that the final **Physical Plan** (which is executed) is identical in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherDF = spark.read.option(\"header\", True).csv(\"./data/policy.csv\") \\\n",
    "               .select(\"policy\",\n",
    "                       \"make\",\n",
    "                       F.col(\"vehicle_age\").cast(IntegerType()).alias(\"vehicle_age\"),\n",
    "                       F.col(\"sum_insured\").cast(IntegerType()).alias(\"sum_insured\"),\n",
    "                       F.to_date(F.col(\"inception_date\"), \"yyyyMMdd\").alias(\"inception_date\"),\n",
    "                       F.to_date(F.col(\"start_date\"), \"yyyyMMdd\").alias(\"inception_date\"),\n",
    "                       F.to_date(F.col(\"end_date\"), \"yyyyMMdd\").alias(\"inception_date\"),\n",
    "                       F.col(\"premium\").cast(IntegerType()).alias(\"premium\")\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be tested to be the same as the original policyDF DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert policyDF.collect() == otherDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the output of the `explain()` now (as shown below) it can be seen that the **Optimized Logical Plan** and **Physical Plan** are identical to the example for the `policyDF` DataFrame, however the initial **Parsed Logical Plan** is much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('policy, None), unresolvedalias('make, None), cast('vehicle_age as int) AS vehicle_age#148, cast('sum_insured as int) AS sum_insured#149, to_date('inception_date, Some(yyyyMMdd)) AS inception_date#150, to_date('start_date, Some(yyyyMMdd)) AS inception_date#151, to_date('end_date, Some(yyyyMMdd)) AS inception_date#152, cast('premium as int) AS premium#153]\n",
      "+- Relation[policy#132,make#133,vehicle_age#134,sum_insured#135,inception_date#136,start_date#137,end_date#138,premium#139] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "policy: string, make: string, vehicle_age: int, sum_insured: int, inception_date: date, inception_date: date, inception_date: date, premium: int\n",
      "Project [policy#132, make#133, cast(vehicle_age#134 as int) AS vehicle_age#148, cast(sum_insured#135 as int) AS sum_insured#149, to_date('inception_date, Some(yyyyMMdd)) AS inception_date#150, to_date('start_date, Some(yyyyMMdd)) AS inception_date#151, to_date('end_date, Some(yyyyMMdd)) AS inception_date#152, cast(premium#139 as int) AS premium#153]\n",
      "+- Relation[policy#132,make#133,vehicle_age#134,sum_insured#135,inception_date#136,start_date#137,end_date#138,premium#139] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [policy#132, make#133, cast(vehicle_age#134 as int) AS vehicle_age#148, cast(sum_insured#135 as int) AS sum_insured#149, cast(cast(unix_timestamp(inception_date#136, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#150, cast(cast(unix_timestamp(start_date#137, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#151, cast(cast(unix_timestamp(end_date#138, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#152, cast(premium#139 as int) AS premium#153]\n",
      "+- Relation[policy#132,make#133,vehicle_age#134,sum_insured#135,inception_date#136,start_date#137,end_date#138,premium#139] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [policy#132, make#133, cast(vehicle_age#134 as int) AS vehicle_age#148, cast(sum_insured#135 as int) AS sum_insured#149, cast(cast(unix_timestamp(inception_date#136, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#150, cast(cast(unix_timestamp(start_date#137, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#151, cast(cast(unix_timestamp(end_date#138, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#152, cast(premium#139 as int) AS premium#153]\n",
      "+- FileScan csv [policy#132,make#133,vehicle_age#134,sum_insured#135,inception_date#136,start_date#137,end_date#138,premium#139] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/michael/pyspark-tutorial/data/policy.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<policy:string,make:string,vehicle_age:string,sum_insured:string,inception_date:string,star...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "otherDF.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the code to simplify the **Parsed Logical Plan** can both assist the Optimiser, and also improve your ability to understand the steps that Spark will take.\n",
    "\n",
    "The loss of the generic `fix_dates` function from previous examples can be replaced with another generic function, which builds the `select()` rather than a sequence of `withColumn()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Fixing integer column 'vehicle_age'.\n",
      "NOTE: Fixing integer column 'sum_insured'.\n",
      "NOTE: Fixing date column 'inception_date'.\n",
      "NOTE: Fixing date column 'start_date'.\n",
      "NOTE: Fixing date column 'end_date'.\n",
      "NOTE: Fixing integer column 'premium'.\n"
     ]
    }
   ],
   "source": [
    "def fix_types(df):\n",
    "    \"\"\"Update columns to the correct Spark Date type.\"\"\"\n",
    "    sel = []\n",
    "    \n",
    "    intcols = (\"premium\", \"sum_insured\", \"vehicle_age\")\n",
    "                    \n",
    "    for column in df.columns:\n",
    "        if column in intcols:\n",
    "            print(\"NOTE: Fixing integer column '{}'.\".format(column))\n",
    "            sel.append(F.col(column).cast(IntegerType()).alias(column))\n",
    "        elif column.endswith(\"_date\") and dict(df.dtypes)[column] == 'string':\n",
    "            print(\"NOTE: Fixing date column '{}'.\".format(column))\n",
    "            sel.append(F.to_date(F.col(column), \"yyyyMMdd\").alias(column))\n",
    "        else:\n",
    "            sel.append(column)\n",
    "\n",
    "    return df.select(sel)\n",
    "\n",
    "otherDF = spark.read.option(\"header\", True).csv(\"./data/policy.csv\")\n",
    "otherDF = fix_types(otherDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of generating a `select()` results in a single 'Project' layer in the **Parsed Logical Plan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('policy, None), unresolvedalias('make, None), cast('vehicle_age as int) AS vehicle_age#210, cast('sum_insured as int) AS sum_insured#211, to_date('inception_date, Some(yyyyMMdd)) AS inception_date#212, to_date('start_date, Some(yyyyMMdd)) AS start_date#213, to_date('end_date, Some(yyyyMMdd)) AS end_date#214, cast('premium as int) AS premium#215]\n",
      "+- Relation[policy#194,make#195,vehicle_age#196,sum_insured#197,inception_date#198,start_date#199,end_date#200,premium#201] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "policy: string, make: string, vehicle_age: int, sum_insured: int, inception_date: date, start_date: date, end_date: date, premium: int\n",
      "Project [policy#194, make#195, cast(vehicle_age#196 as int) AS vehicle_age#210, cast(sum_insured#197 as int) AS sum_insured#211, to_date('inception_date, Some(yyyyMMdd)) AS inception_date#212, to_date('start_date, Some(yyyyMMdd)) AS start_date#213, to_date('end_date, Some(yyyyMMdd)) AS end_date#214, cast(premium#201 as int) AS premium#215]\n",
      "+- Relation[policy#194,make#195,vehicle_age#196,sum_insured#197,inception_date#198,start_date#199,end_date#200,premium#201] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [policy#194, make#195, cast(vehicle_age#196 as int) AS vehicle_age#210, cast(sum_insured#197 as int) AS sum_insured#211, cast(cast(unix_timestamp(inception_date#198, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#212, cast(cast(unix_timestamp(start_date#199, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#213, cast(cast(unix_timestamp(end_date#200, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#214, cast(premium#201 as int) AS premium#215]\n",
      "+- Relation[policy#194,make#195,vehicle_age#196,sum_insured#197,inception_date#198,start_date#199,end_date#200,premium#201] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [policy#194, make#195, cast(vehicle_age#196 as int) AS vehicle_age#210, cast(sum_insured#197 as int) AS sum_insured#211, cast(cast(unix_timestamp(inception_date#198, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS inception_date#212, cast(cast(unix_timestamp(start_date#199, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS start_date#213, cast(cast(unix_timestamp(end_date#200, yyyyMMdd, Some(Australia/Sydney)) as timestamp) as date) AS end_date#214, cast(premium#201 as int) AS premium#215]\n",
      "+- FileScan csv [policy#194,make#195,vehicle_age#196,sum_insured#197,inception_date#198,start_date#199,end_date#200,premium#201] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/michael/pyspark-tutorial/data/policy.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<policy:string,make:string,vehicle_age:string,sum_insured:string,inception_date:string,star...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "otherDF.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
